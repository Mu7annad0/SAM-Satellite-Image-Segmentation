{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m points \u001b[38;5;241m=\u001b[39m (coords_torch, labels_torch)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     60\u001b[0m                 sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[0;32m---> 61\u001b[0m                 points \u001b[38;5;241m=\u001b[39m \u001b[43mpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device),\n\u001b[1;32m     62\u001b[0m                 boxes \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     63\u001b[0m                 masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m                 pred_masks, iou_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmask_decoder(\n\u001b[1;32m     66\u001b[0m                     image_embeddings \u001b[38;5;241m=\u001b[39m image_embedding\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     67\u001b[0m                     image_pe \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprompt_encoder\u001b[38;5;241m.\u001b[39mget_dense_pe(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     multimask_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m                 )\n\u001b[1;32m     72\u001b[0m pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(pred_masks,size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m512\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******interpolate\n",
      "*******load ../workdir/ck/SAM_checkpoint/sam_vit_b_01ec64.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mu7annad.0gmail.com/Documents/Project/SAM-Satellite-Image-Segmentation/DataLoader.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image).float(),\n",
      "/Users/mu7annad.0gmail.com/Documents/Project/SAM-Satellite-Image-Segmentation/DataLoader.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask).float(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU: 0.05095195397734642, DICE: 0.09674651175737381\n",
      "loss: -1.120917797088623\n",
      "shape of sparse_embeddings torch.Size([2, 7, 256])\n",
      "shape of dense_embeddings torch.Size([2, 256, 32, 32])\n",
      "shape of image embedding torch.Size([2, 256, 32, 32])\n",
      "shape of pred_masks torch.Size([2, 1, 512, 512])\n",
      "shape of masks torch.Size([2, 1, 512, 512])\n",
      "shape of image torch.Size([2, 3, 512, 512])\n",
      "shape of box torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from DataLoader import RoadDataset\n",
    "import argparse\n",
    "from cfg import parse_args\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from utils import FocalDiceloss_IoULoss, SegMetrics\n",
    "import einops\n",
    " \n",
    "\n",
    "ckeckpoint_dir = '../workdir/ck/SAM_checkpoint/sam_vit_b_01ec64.pth'\n",
    "device = torch.device(\"mps\")\n",
    "train_root = \"../../graduating_project/Dataset/DeepGlobeRoadExtraction/road/train/\"\n",
    "\n",
    "# model_save_path = os.path.join(args.work_dir, args.run_name)\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "dataset = RoadDataset(data_root=train_root, image_size=512, train=True, box = True, points=True, transform=True)\n",
    "train_dataloader = DataLoader(dataset, 2, True)\n",
    "model = sam_model_registry['vit_b'](checkpoint=ckeckpoint_dir).to(device)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    image = batch['image'].to(device)\n",
    "    mask = batch['mask'].to(device)\n",
    "\n",
    "    box = batch['box']\n",
    "    point_coords = batch[\"point_coords\"]\n",
    "    point_labels = batch[\"point_labels\"]\n",
    "    break\n",
    "# sam_trans = ResizeLongestSide(model.image_encoder.img_size)\n",
    "# box = np.asarray(box)\n",
    "# box = sam_trans.apply_boxes(box, (mask.shape[-2], mask.shape[-1]))\n",
    "# box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "# if len(box_torch.shape) == 2:\n",
    "#                 box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "for n, value in model.image_encoder.named_parameters():\n",
    "    if \"Adapter\" in n:\n",
    "           value.requires_grad = True\n",
    "    else:\n",
    "            value.requires_grad = False\n",
    "\n",
    "image_embedding = model.image_encoder(image)\n",
    "\n",
    "coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=device)\n",
    "labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=device)\n",
    "\n",
    "points = (coords_torch, labels_torch)\n",
    "with torch.no_grad():\n",
    "                sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                points = points,\n",
    "                boxes = box.to(device),\n",
    "                masks = None)\n",
    "\n",
    "                pred_masks, iou_pred = model.mask_decoder(\n",
    "                    image_embeddings = image_embedding.to(device),\n",
    "                    image_pe = model.prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings = sparse_embeddings,\n",
    "                    dense_prompt_embeddings = dense_embeddings,\n",
    "                    multimask_output = False\n",
    "                )\n",
    "pred = F.interpolate(pred_masks,size=(512,512))\n",
    "\n",
    "seg_loss = FocalDiceloss_IoULoss()\n",
    "mask = einops.repeat(mask, \"b h w -> b 1 h w\")\n",
    "loss = seg_loss(pred, mask, iou_pred)\n",
    "\n",
    "metric = SegMetrics(pred, mask, ['iou', 'dice'])\n",
    "print(f\"IOU: {metric[0]}, DICE: {metric[1]}\")\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"shape of sparse_embeddings {sparse_embeddings.shape}\")\n",
    "print(f\"shape of dense_embeddings {dense_embeddings.shape}\")\n",
    "print(f\"shape of image embedding {image_embedding.shape}\")\n",
    "print(f\"shape of pred_masks {pred.shape}\")\n",
    "print(f\"shape of masks {mask.shape}\")\n",
    "print(f\"shape of image {image.shape}\")\n",
    "print(f\"shape of box {box.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_point\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m):\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m     mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "from utils import generate_point\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    image = batch['image'].to(device)\n",
    "    mask = batch['mask'].to(device)\n",
    "\n",
    "\n",
    "points = generate_point(pred, mask, pred_masks, batch, 5)\n",
    "\n",
    "with torch.no_grad():\n",
    "                sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                points = points,\n",
    "                boxes = box.to(device),\n",
    "                masks = None)\n",
    "\n",
    "                pred_masks, iou_pred = model.mask_decoder(\n",
    "                    image_embeddings = image_embedding.to(device),\n",
    "                    image_pe = model.prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings = sparse_embeddings,\n",
    "                    dense_prompt_embeddings = dense_embeddings,\n",
    "                    multimask_output = False\n",
    "                )\n",
    "\n",
    "pred = F.interpolate(pred_masks,size=(1024,1024))\n",
    "seg_loss = FocalDiceloss_IoULoss()\n",
    "mask = einops.repeat(mask, \"b h w -> b 1 h w\")\n",
    "loss = seg_loss(pred, mask, iou_pred)\n",
    "\n",
    "metric = SegMetrics(pred, mask, ['iou', 'dice'])\n",
    "print(metric)\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"shape of sparse_embeddings {sparse_embeddings.shape}\")\n",
    "print(f\"shape of dense_embeddings {dense_embeddings.shape}\")\n",
    "print(f\"shape of image embedding {image_embedding.shape}\")\n",
    "print(f\"shape of pred_masks {pred.shape}\")\n",
    "print(f\"shape of masks {mask.shape}\")\n",
    "print(f\"shape of image {image.shape}\")\n",
    "print(f\"shape of box {box.shape}\")\n",
    "print(f\"shape of points {points.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=0.001, weight_decay=0)\n",
    "loss = seg_loss(pred, mask, iou_pred)\n",
    "model_save_path = os.path.join('../workdir/', 'sam-satellite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/127 [00:14<10:19,  5.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     15\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name\n\u001b[0;32m---> 17\u001b[0m image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# do not compute gradients for image encoder and prompt encoder\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# convert box to 1024x1024 grid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Project/SAM-Satellite-Image-Segmentation/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Project/SAM-Satellite-Image-Segmentation/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Project/SAM-Satellite-Image-Segmentation/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Project/SAM-Satellite-Image-Segmentation/segment_anything/modeling/image_encoder.py:350\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[1;32m    349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[0;32m--> 350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    353\u001b[0m r_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mreshape(B, q_h, q_w, dim)\n",
      "File \u001b[0;32m~/Documents/Project/SAM-Satellite-Image-Segmentation/segment_anything/modeling/image_encoder.py:292\u001b[0m, in \u001b[0;36mget_rel_pos\u001b[0;34m(q_size, k_size, rel_pos)\u001b[0m\n\u001b[1;32m    288\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[:, :H, :W, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rel_pos\u001b[39m(q_size: \u001b[38;5;28mint\u001b[39m, k_size: \u001b[38;5;28mint\u001b[39m, rel_pos: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Get relative positional embeddings according to the relative positions of\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        query and key sizes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m        Extracted positional embeddings according to relative positions.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     max_rel_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_size, k_size) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    epoch_loss = 0\n",
    "    # Just train on the first 20 examples\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        image = batch['image'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "\n",
    "        if 'box' in batch:\n",
    "            box = batch['box']\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "                param.requires_grad = \"image_encoder\" not in name\n",
    "        \n",
    "        image_embedding = model.image_encoder(image)\n",
    "\n",
    "        # do not compute gradients for image encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            # convert box to 1024x1024 grid\n",
    "            sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                    points = None,\n",
    "                    boxes = box.to(device),\n",
    "                    masks = None)\n",
    "            \n",
    "        pred_masks, iou_pred = model.mask_decoder(\n",
    "                    image_embeddings = image_embedding.to(device),\n",
    "                    image_pe = model.prompt_encoder.get_dense_pe(),\n",
    "                    sparse_prompt_embeddings = sparse_embeddings,\n",
    "                    dense_prompt_embeddings = dense_embeddings,\n",
    "                    multimask_output = False\n",
    "            )       \n",
    "        \n",
    "        pred = F.interpolate(pred_masks,size=(1024, 1024))\n",
    "\n",
    "            # to add the mask to the loss function it need to be shape of [B, 1, H, W] (1)\n",
    "            # but it in shape of [B, H, W] (2), so the blow line convert from shape 2 --> 1\n",
    "        mask = einops.repeat(mask, \"b h w -> b 1 h w\")\n",
    "        loss = seg_loss(pred, mask, iou_pred)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= step\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
    "    # save the model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, 'sam_model_latest.pth'))\n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_path, 'sam_model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 127/127 [08:05<00:00,  3.82s/it, loss=-144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Loss: -144.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 127/127 [08:03<00:00,  3.80s/it, loss=-147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: -147.1673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_sam(propmt_grad, model, optimizer, criterion, train_loader, device, epoch, no_epochs=2):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(no_epochs):\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch}') as pbar:\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "                    image = batch['image'].to(device)\n",
    "                    mask = batch['mask'].to(device)\n",
    "\n",
    "                    if 'box' in batch:\n",
    "                        box = batch['box']\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    for name, param in model.named_parameters():\n",
    "                        param.requires_grad = \"image_encoder\" not in name\n",
    "\n",
    "                    image_embedding = model.image_encoder(image)\n",
    "\n",
    "                    if propmt_grad is True:\n",
    "                        sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                        points = None,\n",
    "                        boxes = box.to(device),\n",
    "                        masks = None\n",
    "                    )\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "                            points = None,\n",
    "                            boxes = box.to(device),\n",
    "                            masks = None\n",
    "                    )\n",
    "                    pred_masks, iou_pred = model.mask_decoder(\n",
    "                            image_embeddings = image_embedding.to(device),\n",
    "                            image_pe = model.prompt_encoder.get_dense_pe(),\n",
    "                            sparse_prompt_embeddings = sparse_embeddings,\n",
    "                            dense_prompt_embeddings = dense_embeddings,\n",
    "                            multimask_output = False\n",
    "                    )       \n",
    "                    \n",
    "                    # The pred_masks in shape of [B, C, 256, 256] we need to upsample or downsample to the img input size \n",
    "                    # So the below line will ## Resize to the ordered output size\n",
    "                    pred = F.interpolate(pred_masks,size=(1024, 1024))\n",
    "\n",
    "                    # to add the mask to the loss function it need to be shape of [B, 1, H, W] (1)\n",
    "                    # but it in shape of [B, H, W] (2), so the blow line convert from shape 2 --> 1\n",
    "                    mask = einops.repeat(mask, \"b h w -> b 1 h w\")\n",
    "\n",
    "                    loss = criterion(pred, mask, iou_pred)\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix({'loss': epoch_loss / (batch_idx + 1)})\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        print(f'Epoch {epoch}/{1}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "dataset = RoadDataset(data_root=train_root, image_size=1024, train=True, box = True, transform=True)\n",
    "train_dataloader = DataLoader(dataset, 2, True)\n",
    "model = sam_model_registry['vit_b'](checkpoint=ckeckpoint_dir).to(device)\n",
    "train_sam(True, model, optimizer, seg_loss, train_dataloader, device, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
